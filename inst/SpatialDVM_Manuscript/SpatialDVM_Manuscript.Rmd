---
title: A comparison of design-based and model-based approaches for spatial data.
author:
  - name: Michael Dumelle
    email: Dumelle.Michael@epa.gov
    affiliation: USEPA
    footnote: 1
  - name: Matthew Higham
    email: mhigham@stlaw.edu
    affiliation: STLAW
    footnote: 1
  - name: Lisa Madsen
    affiliation: OSU
  - name: Anthony R. Olsen
    affiliation: USEPA
  - name: Jay M. Ver Hoef
    affiliation: NOAA
address:
  - code: USEPA
    address: United States Environmental Protection Agency, 200 SW 35th St, Corvallis, Oregon, 97333
  - code: STLAW
    address: Saint Lawrence University Department of Math, Computer Science, and Statistics, 23 Romoda Drive, Canton, New York, 13617
  - code: OSU
    address: Oregon State University Department of Statistics, 239 Weniger Hall, Corvallis, Oregon, 97331
  - code: NOAA
    address: Marine Mammal Laboratory, Alaska Fisheries Science Center, National Oceanic and Atmospheric Administration, Seattle, Washington, 98115
footnote:
  - code: 1
    text: "Corresponding Author"
abstract: |
  This is the abstract.

  It consists of two paragraphs.

journal: "An awesome journal"
date: "`r Sys.Date()`"
bibliography: mybibfile.bib
#linenumbers: true
numbersections: true
csl: elsevier-harvard.csl
preamble: >
  \usepackage{bm}
  \usepackage{bbm}
  \usepackage{color}
  \DeclareMathOperator{\var}{{var}}
  \DeclareMathOperator{\cov}{{cov}}
output: rticles::elsevier_article
---

_Text based on elsarticle sample manuscript, see [http://www.elsevier.com/author-schemas/latex-instructions#elsarticle](http://www.elsevier.com/author-schemas/latex-instructions#elsarticle)_


# Introduction {#sec:introduction}
 
finish spatial introduction
revise section 2
reread brus and wang
write down potential data sets

Please leave comments in your \textcolor{blue}{c}\textcolor{magenta}{o}\textcolor{green}{l}\textcolor{red}{o}\textcolor{cyan}{r}: \textcolor{blue}{Michael}, \textcolor{magenta}{Matt}, \textcolor{green}{Lisa}, \textcolor{red}{Tony}, \textcolor{cyan}{Jay}.

<!-- Brief introduction to model based and design based -->

There are two general approaches for using data to make statistical inferences about a population: design-based approaches and model-based approaches. When data cannot be obtained for all units in a population (known as population units), data on a subset of the population units is collected in a  sample. In the design-based approach, inferences about the underlying population are informed from a probabilistic process in which population units are selected to be in the sample. Alternatively, in the model-based approach, inferences are made from specific assumptions made about the underlying process that generated the data. Each paradigm has a deep historical context [@sterba2009alternative] and its own set of general advantages [@hansen1983evaluation].

<!-- Shift focus to spatial data -->

Though the design-based and model-based approaches apply to statistical inference in a broad sense, we focus on comparing these approaches for spatial data. We define spatial data as variables measured at specific geographic locations. @de1990model give an early comparison of design-based and model-based approaches for spatial data, quashing the belief that design-based approaches could not be used for spatially correlated data. Thereafter, several comparisons between design-based and model-based for spatial data have been considered, but they tend to compare design-based approaches that ignore spatial location in sampling to model-based approaches [@brus1997random; @verhoef2002sampling; @verhoef2008spatial]. More recent overviews include @brus2020statistical and @wang2012spatial, but no numerical comparison has been made between design-based approaches that incorporate spatial location in sampling and model-based approaches.

<!-- Outline for the rest of the paper -->

The rest of this paper is organized as follows. In Section \ref{sec:background}, we compare sampling and estimation procedures between the design-based approach and the model-based approach. In Section \ref{sec:numstudy}, we use simulated and real data to study the properties of parameter estimates from both approaches. And in Section \ref{sec:discussion}, we end with a discussion and provide directions for future research. 

# Background {#sec:background}

## Design-Based Philosphy

Design-Based Overview 

<!-- \citep{Sarndal2003model, Lohr2009sampling} -->

The design-based approach to inference uses characteristics of the sampling design to estimate parameters of interest. The design-based approach assumes the data, $\mathbf{Z}$, are fixed. Randomness is incorporated through the random selection of sampling units based on probabilities derived using the sampling design itself. Some examples of commonly used sampling designs include independent random sampling (IRS), stratified random sampling, and cluster sampling. Viewing the data as fixed and incorporating randomness through the sampling design yields estimators having very few assumptions. Asymptotic properties of these types of estimators are derived using limiting arguments. Means and totals, for example, are asymptotically Gaussian. @sarndal2003model and @lohr2009sampling provide thorough reviews of the design-based approach.

## Model-Based Philosphy

Model-Based Overview

<!-- \citep{Cressie2015statistics, Schabenberger2017statistical} -->

On the other hand, model-based inference imposes additional assumptions on the data with a potential to provide more precise estimators if the additional assumptions hold. Instead of estimating true but unknown parameters, the goal of model-based inference in the spatial context is often \emph{prediction} of an unknown quantity. This is a fundamental philosophical difference between sampling-based and model-based approaches. Instead of \emph{estimating} a fixed unknown mean, we are \emph{predicting} the value of the mean, a random variable. We know that if we sampled all sites, we would have an exact prediction for the mean of our one realized spatial surface, without any uncertainty. But, the true mean of the spatial process that generated our realized data is still not known, and, in the prediction context, we typically do not care much about what value the mean of the underlying process takes.

We only use FPBK in this paper in order to focus more on comparing the design-based and model-based approaches. However, k-nearest-neighbors [@fix1951discriminatory; @ver2013comparison], random forest [@breiman2001random], Bayesian models [@chan2020bayesian], among others, can also be used to obtain predictions for a mean or total from spatially correlated responses in a finite population setting.

Figure 1a. Data is fixed. In a finite population example, show a 3d surface that can be generated by anything. If we repeatedly sample the surface, then 95% of all 95% CIs will contain the true mean, which never changes.

Figure 1b. Spatial process is fixed. In a finite population example, show 10 3d surfaces that are generated from some model. If we repeatedly generate the surface and obtain a sample, then 95% of all 95% PIs will contain the realized means. The realized mean changes from surface to surface and it's not necessarily the case that 95% of all 95% PIs will contain the true, underlying mean.

## Comparing Design-Based vs. Model-Based

Several comparisons between the two paradigms have been made in many different contexts. @sterba2009alternative give some history of the two paradigms as well as some applications in psychological contexts. @cooper2006sampling review the two approaches in an ecological context before introducing a "model-assisted" variance estimator that combines aspects from each approach. We note that while there has been substantial research and development into estimators that use both design and model-based principles (see e.g. @cicchitelli2012model, @chan2020bayesian for a Bayesian approach, CITE some more of these), our goal is not to expand upon or improve these methods.

## Spatially Balanced Design and Analysis

Spatially balanced sampling algorithms use spatial information to obtain samples spread out in space. Spatially balanced samples are useful because they tend to yield estimators that are more precise than estimators constructed from a sampling algorithm that is not spatially balanced [@stevens2004spatially; @barabesi2011sampling; @grafstrom2013well; @robertson2013bas; @benedetti2017spatiallyreview; @wang2013design]. To quantify spatial balance, @stevens2004spatially proposed statistics based on Voroni polygons. Many spatially balanced sampling algorithms exist, including the Generalized Random Tessellation Stratified [@stevens2004spatially], the Local Pivotal Method [@grafstrom2012spatially; @grafstrom2018spatially], Spatially Correlated Poisson Sampling [@grafstrom2012spatiallypoisson], Balanced Acceptance Sampling [@robertson2013bas], Within-Sample-Distance [@benedetti2017spatially], and Halton Iterative Partitioning [@robertson2018halton] algorithms. Here we focus on the Generalized Random Tessellation Stratified (GRTS) algorithm, which has several attractive properties that we discuss next. 

The GRTS algorithm is used to sample from finite and infinite sample frames. A finite sample frame contains a finite number of sampling units and is related to a point geometry. An infinite sample frame contains an infinite number of sampling units and is related to linear and polygon geometries. Examples of point, linear, and polygon resources include lake centroids, stream networks, and wetland areas, respectively. In addition to its applicability for finite and infinite sample frames, the GRTS algorithm naturally accommodates stratified designs and designs with unequal selection probabilities. The algorithm has also been used to select replacement sites using reverse hierarchical ordering [@stevens2004spatially]. Replacement sites are used to replace sites in the original sample that cannot be sampled, often as a result of physical difficulty in reaching the site or landowner denial of access to the sites. More recently, the GRTS algorithm also accommodates legacy (historical) sites, minimum distance between sites, and nearest neighbor replacement sites. The GRTS algorithm is implemented in the \textbf{\textsf{R}} package \texttt{spsurvey} [@dumelle2021spsurvey]. 

The GRTS algorithm works by mapping two-dimensional space into one-dimensional space while incorporating some elements of randomness. First a square bounding box is placed over the sample frame and divided into four equally sized squares randomly given an level-one address of 0, 1, 2, or 3. Then the inclusion probabilities of the sampling units in each square are summed. If this sum is at least one in any of the cells, a set of four sub-squares are placed within each square. Each set of four sub-squares are randomly given a second address (a level-two address) of 0, 1, 2, 3. Then the inclusion probabilities in each sub-square are summed, and if any of these sums are at least one, a third level of squares are added in the same manner. This hierarchical addressing continues until the inclusion probabilities in the smallest squares are all less than 1. The squares are then mapped to the one-dimensional line in order of their addresses. The length of this line equals $n$, the desired sample size (the sum of the inclusion probabilities). A uniform random variable, denoted by $u_1$ is simulated in $[0, 1]$ and placed on this line. The location of $u_1$ on the line can be linked to a sampling unit, denoted by $s_1$. Then $u_2 \equiv u_1 + 1$, which can be linked to another sampling unit, denoted by $s_2$. Because the GRTS algorithm requires the inclusion probabilities of the smallest sub-cells to be less than one, $s_1$ and $s_2$ must be distinct. This process continues until the $n$ sampling units have been selected as part of the sample.  Further details are provided by [@stevens2004spatially] and [@dumelle2021spsurvey].

The Horvitz-Thompson estimator [@horvitz1952generalization] and its continuous analog [@cordy1993extension] yield unbiased estimates of population totals (and means). If $\tau$ is a population total, then the Horvitz-Thompson estimator of $\tau$, denoted by $\hat{\tau}_{HT}$, is given by
\begin{align}\label{eq:htest}
  \hat{\tau}_{HT} = \sum_{i = 1}^n Z_i \pi_i^{-1},
\end{align}
where $Z_i$ is the observed value of the $i$th sampling unit, and $\pi_i$ is the inclusion probability of the $i$th sampling unit. Estimating the variance of $\hat{\tau}_{HT}$ directly relies on knowing the probability that $s_i$ and $s_j$ are both included in the sample (second-order inclusion probability), which can be unfeasible to compute. Furthermore, this variance estimator does not incorporate spatial locations. To address these challenges, [@stevens2003variance] proposed an alternative variance estimator: the local neighborhood variance estimator. The local neighborhood variance estimator does not require second-order inclusion probabilities. It also incorporates spatial locations, which tends to reduce the estimated variance of compared $\hat{\tau}_{HT}$ to a variance estimator ignoring spatial location [@stevens2003variance]. 

## Finite Population Block Kriging

<!-- \citep{VerHoef2002sampling, VerHoef2008spatial, Higham2020adjusting} -->

Finite Population Block Kriging (FPBK) is an alternative to samipling-based methods [@verhoef2008spatial]. FPBK expands the geostatistical kriging framework to the finite population setting. Instead of basing inference off of a specific sampling design, we assume the data were generated by a spatial process with parameters that can be estimated using the framework of a model. 

@verhoef2008spatial gives details on the theory of FPBK, but some of the basic principles are summarized below. For a response variable $\mathbf{z}$ that can be measured on a finite number of $N$ sites, we want to predict some linear function of the response variable, $\tau(\mathbf{z}) = \mathbf{b}^\prime \mathbf{z}$, where $\mathbf{b}$ is a vector of weights. For example, if we want to predict the total abundance across all sites, then we would use a vector of 1's for the weights. 

Typically, however, we only have a sample of the $N$ sites. Denoting quantities that are part of the sampled sites with a subscript \emph{s} and quantities that are part of the unsampled sites with a subscript \emph{u}, 

\begin{equation}
\begin{pmatrix} \label{equation:Zmarginal}
    \mathbf{z}_s      \\
    \mathbf{z}_u
\end{pmatrix}
=
\begin{pmatrix}
  \mathbf{X}_s    \\
  \mathbf{X}_u
\end{pmatrix}
\bm{\beta} +
\begin{pmatrix}
\bm{\delta}_s    \\
\bm{\delta}_u
\end{pmatrix},
\end{equation}
where $\mathbf{X}_s$ and $\mathbf{X}_u$ are the design matrices for the sampled and unsampled sites, respectively, and $\bm{\delta}_s$ and $\bm{\delta}_u$ are random errors for the sampled and unsampled sites. Denoting $\bm{\delta} \equiv [\bm{\delta}_s \,\, \bm{\delta}_u]'$, we assume that $E(\bm{\delta})$ = $\mathbf{0}$.

We also typically assume that there is spatial correlation in $\bm{\delta}$, which can be modeled using a covariance function. Many common choices for this function assume that spatial covariance decreases with increasing Euclidean distance between sites. The primary function used throughout the simulations and applications of this manuscript is the Exponential covariance function: the $i,j^{th}$ entry for $\var(\bm{\delta})$ is
\mbox{}
\begin{align}\label{equation:expcov}
\cov(\delta_i, \delta_j) = \theta_1\exp(-3h_{i,j}/\theta_2) + \theta_3\mathbbm{1}\{\mathbf{h}_{i,j} = 0\}, 
\end{align}
where $h_{i,j}$ is the distance between sites $i$ and $j$, and $\bm{\theta}$ is a vector of spatial covariance parameters of the partial sill $\theta_1$, the range $\theta_2$, and the nugget $\theta_3$. However, any spatial covariance function could be used in the place of the Exponential, including functions that allow for anisotropy [pg. 80 - 93][@chiles1999geostatistics].

With the above model formulation, the Best Linear Unbiased Predictor (BLUP) for $\tau(\mathbf{b}'\mathbf{z})$ $\tau(\mathbf{b}'\mathbf{z})$ is vague} and its prediction variance can be computed. While details of the derivation are in [@verhoef2008spatial], we note here that the predictor and its variance are both moment-based. Neither require a particular distribution for $\mathbf{z}$.

# Numerical Study {#sec:numstudy}

cases

__Base Simulations__

* both good: correctly specified model with high correlation
* break model: non-gaussian errors
* break design: small area estimation

* both good?: misspecified covariance model with high correlation
* break both? non-gaussian areas with smaller sample size

* change n or sampling fraction

* model-based: how should sample be drawn?

## Software

FPBK can be readily performed in `R` with the `sptotal` package [@higham2020sptotal]. We use `sptotal` for both the simulation analysis and the application, estimating parameters with Restricted Maximum Likelihood (REML).

## Applied Example

# Discussion {#sec:discussion}

<!-- References from Brus2020statistical, de1990model, Brus1997random -->

# References {#references .unnumbered}

