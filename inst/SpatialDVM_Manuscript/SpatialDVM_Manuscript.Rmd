---
title: A comparison of design-based and model-based approaches for spatial data.
author:
  - name: In alphabetical order Michael Dumelle
    email: Dumelle.Michael@epa.gov
    affiliation: USEPA
    footnote: 1
  - name: Matt Higham
    email: mhigham@stlaw.edu
    affiliation: STLAW
    footnote: 1
  - name: Lisa Madsen
    affiliation: OSU
  - name: Anthony R. Olsen
    affiliation: USEPA
  - name: Jay M. Ver Hoef
    affiliation: NOAA
address:
  - code: USEPA
    address: United States Environmental Protection Agency, 200 SW 35th St, Corvallis, Oregon, 97333
  - code: STLAW
    address: Saint Lawrence University Department of Math, Computer Science, and Statistics, 23 Romoda Drive, Canton, New York, 13617
  - code: OSU
    address: Oregon State University Department of Statistics, 239 Weniger Hall, Corvallis, Oregon, 97331
  - code: NOAA
    address: Marine Mammal Laboratory, Alaska Fisheries Science Center, National Oceanic and Atmospheric Administration, Seattle, Washington, 98115
footnote:
  - code: 1
    text: "Corresponding Author"
abstract: |
  This is the abstract.

journal: "An awesome journal"
date: "`r Sys.Date()`"
bibliography: mybibfile.bib
linenumbers: true
numbersections: true
csl: elsevier-harvard.csl
preamble: >
  \usepackage{bm}
  \usepackage{bbm}
  \usepackage{color}
  \DeclareMathOperator{\var}{{var}}
  \DeclareMathOperator{\cov}{{cov}}
output: rticles::elsevier_article
---

_Text based on elsarticle sample manuscript, see [http://www.elsevier.com/author-schemas/latex-instructions#elsarticle](http://www.elsevier.com/author-schemas/latex-instructions#elsarticle)_

Potential Journals:

* Ecological Applications
* Methods in Ecology and Evolution
* Journal of Applied Ecology
* Environmetrics
* Environmental and Ecological Statistics

# Introduction {#sec:introduction}

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>", 
  warning = FALSE, 
  message = FALSE,
  include = TRUE, 
  echo = FALSE
)
library(tidyverse)
library(here)
library(xtable) # latex tables
```

<!-- Brief introduction to model based and design based -->

There are two general approaches for using data to make statistical inferences about a population: design-based approaches and model-based approaches. When data cannot be obtained for all units in a population (population units), data on a subset of the population units is collected in a  sample. In the design-based approach, inferences about the underlying population are informed from a probabilistic process in which population units are selected to be in the sample. Alternatively, in the model-based approach, inferences are made from specific assumptions about the underlying process that generated the data. Each paradigm has a deep historical context [@sterba2009alternative] and its own set of general advantages [@hansen1983evaluation].

<!-- Shift focus to spatial data -->

Though the design-based and model-based approaches apply to statistical inference in a broad sense, we focus on comparing these approaches for spatial data. We define spatial data as variables measured at specific geographic locations. @de1990model give an early comparison of design-based and model-based approaches for spatial data, quashing the belief that design-based approaches could not be used for spatially correlated data. Thereafter, several comparisons between design-based and model-based for spatial data have been considered, but they tend to compare design-based approaches that ignore spatial locations to model-based approaches [@brus1997random; @verhoef2002sampling; @verhoef2008spatial]. @cooper2006sampling review the two approaches in an ecological context before introducing a "model-assisted" variance estimator that combines aspects from each approach. In addition to @cooper2006sampling, there has been substantial research and development into estimators that use both design and model-based principles (see e.g. @cicchitelli2012model, @chan2020bayesian for a Bayesian approach, and @sterba2009alternative). More recent overviews include @brus2020statistical and @wang2012review, but no numerical comparison has been made between design-based approaches that incorporate spatial locations and model-based approaches.

<!-- Outline for the rest of the paper -->

The rest of this paper is organized as follows. In Section \ref{sec:background}, we compare sampling and estimation procedures between the design-based approach and the model-based approach. In Section \ref{sec:numstudy}, we use simulated and real data to study the the behavior of both approaches. And in Section \ref{sec:discussion}, we end with a discussion and provide directions for future research. 

# Background {#sec:background}

The design-based and model-based approaches incorporate randomness in fundamentally different ways. In this section, we describe the role of randomness and its effects on subsequent inferences. We then discuss specific inference methods for the design-based and model-based approaches for spatial data. 

## Comparing Design-Based vs. Model-Based

The design-based approach assumes the data are fixed. Randomness is incorporated in the selection of population units according to a sampling design. A sampling design assigns a positive probability of inclusion in the sample (inclusion probability) to each population unit. Some examples of commonly used sampling designs include independent random sampling (IRS), stratified random sampling, and cluster sampling. The goal is to use the sampling design and the sampled data to estimate population parameters like means and totals. These population parameters are typically assumed to be fixed but unknown. 

Treating the data as fixed and incorporating randomness through the sampling design yields estimators having very few other assumptions. Confidence intervals for these types of estimators are typically derived using limiting arguments. Means and totals, for example, are asymptotically normally distributed by the Central Limit Theorem. @sarndal2003model and @lohr2009sampling provide thorough reviews of the design-based approach.

The model-based approach assumes the data are a random realization of a data-generating process. Randomness is often incorporated through distributional assumptions on this process. Instead of estimating fixed but unknown parameters (as in the design-based approach), the goal of model-based inference in the spatial context is often \emph{prediction} of an unknown quantity. For example, suppose the realized mean of all population units is the quantity of interest. Instead of \emph{estimating} a fixed unknown mean, we are \emph{predicting} the value of the mean, a random variable. We know that if we sampled all population units, we would have an exact prediction for the mean of our one realized process, without any uncertainty. But we are typically not interested in the true, unknown mean of the underlying process.

Assuming the data is a realization of a specific data-generating process yields predictors that are linked to distributional assumptions. These distributional assumptions are used to derive prediction intervals. The distributional assumptions allow the prediction intervals to be more precise. @cressie1993statistics and @schabenberger2017statistical provide reviews of model-based approaches for spatial data.

<!-- Figure 1a. Brus (2020): Data is fixed. In a finite population example, show a 3d surface that can be generated by anything. If we repeatedly sample the surface, then 95% of all 95% CIs will contain the true mean, which never changes. -->

```{r fig1, out.width = "100%", fig.cap = "A comparison of sampling under the design-based and model-based frameworks. In the top row, we have one fixed population, and two random samples. In the bottom row, we have two realizations of the same spatial process sampled at the same locations."}
set.seed(08032021)

source(here("R", "sim_pop.R"))
N <- 25
pop_df <- sim_pop(N = N, gridded = TRUE, cortype = "Exponential",
                  psill = 0.9, erange = 1, nugget = 0.1)

n <- 5
samp_df <- pop_df %>% sample_n(n) 
samp_df2 <- pop_df %>% sample_n(n) 

library(cowplot)
d1 <- ggplot(data = pop_df, aes(x = x, y = y)) +
  geom_point(aes(colour = response), size = 4) +
  scale_colour_viridis_c() +
  geom_point(data = samp_df, size = 5.5, shape = 1, 
             show.legend = FALSE, stroke = 1.5) +
  theme_minimal_grid() +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
  ggtitle("Design 1")

d2 <- ggplot(data = pop_df, aes(x = x, y = y)) +
  geom_point(aes(colour = response), size = 4) +
  scale_colour_viridis_c() +
  geom_point(data = samp_df2, size = 5.5, shape = 1, show.legend = FALSE,
             stroke = 1.5) +
  theme_minimal_grid() +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
  ggtitle("Design 2")


pop_df_mod <- sim_pop(N = N, gridded = TRUE, cortype = "Exponential",
                      psill = 0.9, erange = 1, nugget = 0.1)
pop_df_mod2 <- sim_pop(N = N, gridded = TRUE, cortype = "Exponential",
                       psill = 0.9, erange = 1, nugget = 0.1)
indeces <- sample(1:N, size = n, replace = FALSE)
samp_df_mod <- pop_df_mod %>% slice(indeces)
samp_df_mod2 <- pop_df_mod2 %>% slice(indeces)

m1 <- ggplot(data = pop_df_mod, aes(x = x, y = y)) +
  geom_point(aes(colour = response), size = 4) +
  scale_colour_viridis_c() +
  geom_point(data = samp_df_mod, size = 5.5, shape = 1, show.legend = FALSE, stroke = 1.5) +
  theme_minimal_grid() +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
  ggtitle("Model 1")

m2 <- ggplot(data = pop_df_mod2, aes(x = x, y = y)) +
  geom_point(aes(colour = response), size = 4) +
  scale_colour_viridis_c() +
  geom_point(data = samp_df_mod2, size = 5.5, shape = 1, show.legend = FALSE, stroke = 1.5) +
  theme_minimal_grid() +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
  ggtitle("Model 2")

library(gridExtra)
grid.arrange(d1, d2, m1, m2)
```

Description of Figure \ref{fig:fig1} goes here.

<!-- Figure 1b. Spatial process is fixed. In a finite population example, show 10 3d surfaces that are generated from some model. If we repeatedly generate the surface and obtain a sample, then 95% of all 95% PIs will contain the realized means. The realized mean changes from surface to surface and it's not necessarily the case that 95% of all 95% PIs will contain the true, underlying mean. -->

## Spatially Balanced Design and Analysis

The design-based approach can use spatial locations to obtain spatially balanced samples. First we discuss spatial balance with respect to the population [@stevens2004spatially]. A sample is spatially balanced with respect to the population if the sampled population units are a miniature of the population units. A sample is a miniature of the population if the distribution of the sampled population units mirrors the density of all population units. Spatial balance with respect to the population is different than spatial balance with respect to geography. A sample that is spatially balanced with respect to geography is spread out in some type of equidistant manner over geographical space and is not meant to be miniatures of the population. When we refer to spatial balance henceforth, we mean spatial balance with respect to the population. 

Spatially balanced samples are useful because they tend to yield estimates that have lower variance than estimates constructed from sampling designs lacking spatial balance [@stevens2004spatially; @barabesi2011sampling; @grafstrom2013well; @robertson2013bas; @wang2013design; @benedetti2017spatiallyreview]. To quantify spatial balance, @stevens2004spatially proposed loss functions based on Voroni polygons. The first spatially balanced sampling algorithm that saw widespread use was the Generalized Random Tessellation Stratified [@stevens2004spatially]. Since GRTS was developed, several other spatially balanced sampling algorithms have emerged, including the Local Pivotal Method [@grafstrom2012spatially; @grafstrom2018spatially], Spatially Correlated Poisson Sampling [@grafstrom2012spatiallypoisson], Balanced Acceptance Sampling [@robertson2013bas], Within-Sample-Distance [@benedetti2017spatially], and Halton Iterative Partitioning [@robertson2018halton]. We focus on the Generalized Random Tessellation Stratified (GRTS) algorithm to select spatially balanced sampling because it has several attractive properties detailed by @stevens2004spatially and @dumelle2021spsurvey.

<!-- algorithms. -->

The GRTS algorithm is used to sample from finite and infinite populations and works by utilizing a mapping between two-dimensional and one-dimensional space. The population units in two-dimensional space are divided into cells using a hierarchical index. Population units are then mapped to a one-dimensional line via the hierarchical indexing. The line length of each population unit equals its inclusion probability. A systematic sample is conducted on the line and these samples are linked to a population unit in two-dimensional space, which results in the desired sample. @stevens2004spatially provide and @dumelle2021spsurvey provide further details. 

<!-- This makes it seem like HT is being used to estimate population parameters in GRTS? Needs to be in there, but maybe a sentence making it clear that this isn't being used? Or just change the order and talk about Local variance first and then mention these for more general settings? -->

After collecting a sample using the GRTS algorithm, the data are used to estimate population parameters. The Horvitz-Thompson estimator [@horvitz1952generalization] yields unbiased estimates of population means and totals. For example, if $\tau$ is a population total, then the Horvitz-Thompson estimator of $\tau$ (denoted by $\hat{\tau}_{ht}$), is given by
\begin{align}\label{eq:ht}
  \hat{\tau}_{ht} = \sum_{i = 1}^n Z_i \pi_i^{-1},
\end{align}
where $Z_i$ and $\pi_i$ are the observed value and inclusion probability of the $i$th population unit selected in the sample. A similar formula exists for estimating the mean, $\mu$. @horvitz1952generalization and @sen1953estimate provide variance estimators for $\hat{\tau}_{ht}$, but they have two drawbacks. First, they rely on calculating $\pi_{ij}$, the probability that population unit $i$ and population unit $j$ are included in the sample, and this can be very difficult to calculate. Second, they ignore the spatial locations of the population units. To address these drawbacks, @stevens2003variance proposed a local neighborhood variance estimator. The local neighborhood variance estimator does not rely on $\pi_{ij}$, and it incorporates spatial locations by assigning higher weights to nearby observations. @stevens2003variance show this variance estimators tends to reduce the estimated standard error of $\hat{\tau}$, yielding narrower confidence confidence intervals for $\tau$.

## Finite Population Block Kriging

Finite Population Block Kriging (FPBK) is a model-based approach that expands the geostatistical Kriging framework to the finite population setting [@verhoef2008spatial]. Instead of basing inference off of a specific sampling design, we assume the data are generated by a spatial process. @verhoef2008spatial gives details on the theory of FPBK, but some of the basic principles are summarized below. Let $\mathbf{z} \equiv \{\text{z}(s_1), \text{z}(s_2), . . . , \text{z}(s_N) \}$ be a response variable that can be measured at the $N$ population units and is represented as an $N \times 1$ vector. Suppose we want to predict some linear function of the response variable, $f(\mathbf{z}) = \mathbf{b}^\prime \mathbf{z}$, where $\mathbf{b}$ is a $1 \times N$ vector of weights. For example, if we want to predict the population total across all population units, then we would use a vector of 1's for the weights. 

<!-- how explicitly do we want to define \mathbf{z} -->

<!-- finite number of -->

<!-- confusing having two tau's: tau for total and a tau as a function -->

Typically, however, we only have a sample of the $N$ population units. Denoting quantities that are part of the sampled population units with a subscript \emph{s} and quantities that are part of the unsampled population units with a subscript \emph{u}, 

\begin{equation}
\begin{pmatrix} \label{equation:Zmarginal}
    \mathbf{z}_s      \\
    \mathbf{z}_u
\end{pmatrix}
=
\begin{pmatrix}
  \mathbf{X}_s    \\
  \mathbf{X}_u
\end{pmatrix}
\bm{\beta} +
\begin{pmatrix}
\bm{\delta}_s    \\
\bm{\delta}_u
\end{pmatrix},
\end{equation}
where $\mathbf{X}_s$ and $\mathbf{X}_u$ are the design matrices for the sampled and unsampled population units, respectively; $\beta$ is the parameter vector of fixed effects; and $\bm{\delta}_s$ and $\bm{\delta}_u$ are random errors for the sampled and unsampled population units, respectively. Denoting $\bm{\delta} \equiv [\bm{\delta}_s \,\, \bm{\delta}_u]'$, we assume the expectation of $\bm{\delta}$ equals $\mathbf{0}$.

We also typically assume that there is spatial correlation in $\bm{\delta}$, which can be modeled using a covariance function. It is common to assume the covariance function is second-order stationary and isotropic [@cressie1993statistics], and that the spatial covariance decreases as the separation between population units increases. Many spatial covariance functions exist, but the primary function we use throughout the simulations and applications in this manuscript is the exponential covariance function: the $i,j^{th}$ entry for $\cov(\bm{\delta})$ is
\mbox{}
\begin{align}\label{equation:expcov}
\cov(\delta_i, \delta_j) = \theta_1\exp(-3h_{i,j}/\theta_2) + \theta_3\mathbbm{1}\{\mathbf{h}_{i,j} = 0\}, 
\end{align}
where $h_{i,j}$ is the distance between population units $i$ and $j$, and $\bm{\theta}$ is a vector of spatial covariance parameters of the partial sill $\theta_1$, the range $\theta_2$, and the nugget $\theta_3$, and $\mathbbm{1}$ is an indicator function. However, any spatial covariance function could be used in the place of the exponential, including functions that allow for non-stationarity or anisotropy [@chiles1999geostatistics, pp. 80-93].

With the above model formulation, the Best Linear Unbiased Predictor (BLUP) for $f(\mathbf{b}'\mathbf{z})$ and its prediction variance can be computed. While details of the derivation are in [@verhoef2008spatial], we note here that the predictor and its variance are both moment-based.

We note that we only use FPBK in this paper in order to focus more on comparing the design-based and model-based approaches. However, k-nearest-neighbors [@fix1951discriminatory; @ver2013comparison], random forest [@breiman2001random], Bayesian models [@chan2020bayesian], among others, can also be used to obtain predictions for a mean or total from spatially correlated responses in a finite population setting.

# Numerical Study {#sec:numstudy}

```{r, results = "hide", message = FALSE, warning = FALSE}
library(tidyverse)
files <- list.files(path = here("inst", "output"), pattern = "*.csv",
                    full.names = TRUE)

combo_data <- purrr::map_df(files,
                            ~read_csv(.x) %>% mutate(filename = .x)) %>%
  separate(filename, into = c("junk", "error_dep", "dist", "n", "location"),
           sep = "_") %>%
  select(-junk)
combo_data <- combo_data %>% mutate(sim = interaction(error_dep, dist, n, location))

combo_data_grts <- combo_data %>% filter(approach %in% c("Design GRTS", "Model GRTS"))

dep_only <- combo_data_grts %>% filter(error_dep == "deperr")
dep_long <- dep_only %>%
  pivot_wider(names_from = approach, values_from = rmspe, sim) %>%
  mutate(perc = 100 * (`Design GRTS` - `Model GRTS`) / `Model GRTS`)
```

Major Points from August 3 Simulations:

1. In most of the dependent error simulation settings, either all four approahces (IRS-Design, IRS-Model, GRTS-Design, and GRTS-Model) perform equally or GRTS-Design and GRTS-Model outperform IRS-Design and IRS-Model. Exceptions to this are a couple of the settings with very small sample sizes (n = 10), in which the IRS does better than GRTS. In the independent error settings, it usually doesn't matter much which approach is used, which makes sense.

```{r}
ggplot(data = combo_data, aes(x = sim, y = rmspe, colour = approach)) +
  geom_point() +
  coord_flip()
```

2. We will now focus in a bit more on comparing Design-GRTS to Model-GRTS, the two best approaches for any reasonable sample size. In the independent error settings, the two approaches perform very similarly, so those results are omitted in the following graph. In the dependent error settings, using rmspe as the performance criterion, Model-GRTS outperforms Design-GRTS in 12 of the 18 settings, the two approaches perform very similarly in 3 settings, and Design-GRTS outperforms Model-GRTS in 3 settings.

```{r}
ggplot(data = dep_long, aes(x = fct_reorder(sim, perc), y = perc)) +
  geom_point() +
  coord_flip() +
  labs(y = "Percent Change in rmspe (Design - Model) / Model",
       caption = "Percent > 0 means Model has better rmspe")
```

3. Focusing in on the three settings where Design-GRTS outperforms Model-GRTS, we see that, in two of the settings, the log-normal response has a large variance, corresponding to a large right-skew after exponentiation. All three settings have sites in random locations. However, in only one of these settings would we recommend actually using Design-GRTS. In the other two settings, the data are sufficiently skewed that a practitioner should not use either approach, though it is "safer" to use Design-GRTS.

```{r}
design_better <- dep_long %>% filter(perc < -5)
design_better_full <- inner_join(combo_data_grts, design_better, by = c("sim"))
ggplot(data = design_better_full, aes(x = sim, y = coverage, colour = approach)) +
  geom_point() +
  coord_flip() +
  geom_hline(aes(yintercept = 0.95), alpha = 0.5)
```

4. Take-home messages

- In terms of rmspe, a model-based analysis on a GRTS design yields an rmspe similar to or lower than a design-based analysis on a GRTS design, as long as the response variable is not "too skewed." 
- If the response variable is very skewed, then neither analysis is appropriate, but, the design-based analysis is quicker. 
- a spatially balanced GRTS sample outperforms IRS in nearly all dependent error settings, as expected.
- methods that use spatial correlation generally perform better on random location points than they do on gridded points. This makes some intuitive sense because (1) on average, the minimum distance between an unobserved point and its nearest observed neighbor should be lower for random points and (2) the span of the study area is maximized for a grid based on the way that we set up the simulations (with the random points being drawn as uniform random variables within the boundary of the grid). 
- comparison of Design-GRTS and Model-GRTS between two settings with different locations of points, but otherwise the same simulation parameters, should really be done on the same surface realization. One very strange realized response vector could drastically alter the results, especially on the exponentiated log data. In the same way that we compare the four approaches on the same realized data, we should also try to do the same with the locations, if they are of interest. (The realized mean won't be exactly the same but should be close).

__Sample Simulation__

For the following simulation results, we simulated 1040 different gridded populations, each of size 900 (on the unit square) with sample size 150. For the design-based approach, population units were selected via GRTS, the Horvitz-Thompson estimator was used,and the local mean variance was used. For the model-based approach (FPBK), population units were selected via Independent Random Sampling (IRS) and the appropriate prediction and prediction variance formulas were used. 

The response was normally distributed with an exponential covariance function with partial sill of 0.9, effective range of $\sqrt{2}$, and a nugget of 0.1. For model-based, we assumed the correct form of the covariance function (exponential), but estimated the spatial parameters with REML.

```{r, results = "asis", eval = FALSE}
sim_one <- read_csv(here("inst", "output", "sim_one", "summ_output.csv"))
colnames(sim_one) <- c("Approach", "Bias", "RMSE", "MedAE", "Coverage", "PClose", "MedIL")
sim_one_table <- xtable(sim_one, digits = 4, caption = "Approach, mean bias (Bias), root-mean-squared error (RMSE), median absolute error (MedAE), 95 percent interval coverage (Coverage), proportion of times the approach was closer to the true value (PClose), and median interval length (MedIL)", type = "latex", latex.environments = "center", label = "tab:sim_one")
print(sim_one_table, include.rownames = FALSE, comment = FALSE)
```

__Base Simulations__

* both good: correctly specified model with high correlation (we did this in Table \ref{tab:sim_one})
* break model: highly non-normal errors with small sample size
* break design: small area estimation

__Simulation Discussion Questions__

* model-based: how should sample be drawn? should locations be fixed?
* change n or sampling fraction?

__Other Base Settings?__

* both good?: misspecified covariance model with high correlation
* break both? non-gaussian areas with smaller sample size

## Software

The GRTS algorithm and the local neighborhood variance estimator are available in the \textbf{\textsf{R}} package \texttt{spsurvey} [@dumelle2021spsurvey]. FPBK can be readily performed in `R` with the `sptotal` package [@higham2020sptotal]. We use `sptotal` for both the simulation analysis and the application, estimating parameters with Restricted Maximum Likelihood (REML).

# Application

The Environmental Protection Agency (EPA), states, and tribes periodically conduct National Aquatic Research Surveys (NARS) in the United States to assess the water quality of various bodies of water. We will use the 2012 National Lakes Assessment (NLA), which measures various aspects of lake health and quality in lakes in the contiguous United States, to obtain an interval for mean mercury concentration. Although all lakes in the survey were measured in 2012, there may not always be enough time or money to do so. Therefore, we will explore whether or not we can still obtain a relatively precise estimate for the realized mean mercury concentration if we only take a sample of 100 of the 986 lakes.

```{r, message = FALSE, warning = FALSE, echo = FALSE, results = "hide"}
library(tidyverse)
library(here)
library(SpatialDesignVModel)
data(nla)
nla_df
factor(nla_df$TOTALHG_UNITS) ## all same units

nla_df %>% group_by(SITE_ID) %>%
  summarise(nonmiss = sum(!is.na(TOTALHG_RESULT))) %>%
  ungroup() %>% summarise(maxnonmiss = max(nonmiss))
## HG never measured twice at the same site.

nla_df %>% group_by(SITE_ID) %>%
  summarise(nonmiss = sum(!is.na(TOTALHG_RESULT))) %>%
  summarise(sumnomiss = sum(nonmiss == 0))
## 35 sites never have HG measured. These will be dropped as well as the duplicate sites.

## in duplicate sites, fill in the value for RDis_IX
nla_nomiss <- nla_df %>%
  group_by(SITE_ID) %>%
  fill(RDis_IX, .direction = "downup") %>%
  ungroup() %>%
  filter(!is.na(TOTALHG_RESULT)) %>%
  rename(total_hg = "TOTALHG_RESULT")

nla_nomiss_both <- nla_nomiss
## check
nla_nomiss_both %>% group_by(SITE_ID) %>% count() %>%
  ungroup() %>%
  summarise(not1 = sum(n != 1))

## transform coordinates
library(sptotal)
coord_list <- LLtoTM(cm = mean(nla_nomiss_both$INDEX_LON_DD),
                     lat = nla_nomiss_both$INDEX_LAT_DD, 
                     lon = nla_nomiss_both$INDEX_LON_DD)
nla_nomiss_both$xcoords <- coord_list$xy[ ,1]
nla_nomiss_both$ycoords <- coord_list$xy[ ,2]
```

```{r figdata, out.width = "100%", fig.cap = "Population distribution of mercury concentration for 986 lakes in the contiguous United States. Thirty-five lakes were dropped from the analysis because they were missing mercury concentration.", message = FALSE, warning = FALSE}
merc <- ggplot(data = nla_nomiss_both, aes(x = total_hg)) +
  geom_histogram(colour = "black", fill = "white", bins = 15) +
  labs(x = "HG (ng / g)")
# dist <- ggplot(data = nla_nomiss_both, aes(x = RDis_IX)) +
#   geom_histogram(colour = "black", fill = "white", bins = 15) +
#   labs(x = "Lakeshore Disturbance")
merc_map <- ggplot(data = nla_nomiss_both, aes(x = xcoords, y = ycoords)) +
  geom_point(aes(colour = total_hg)) +
  scale_colour_viridis_c() +
  labs(x = "", y = "", colour = "hg") +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank()) +
  coord_quickmap() 
# dist_map <- ggplot(data = nla_nomiss_both, aes(x = xcoords, y = ycoords)) +
#   geom_point(aes(colour = RDis_IX)) +
#   scale_colour_viridis_c() +
#   labs(x = "", y = "", colour = "Dist") +
#   theme(axis.text.x = element_blank(),
#         axis.text.y = element_blank(),
#         axis.ticks = element_blank())
library(gridExtra)
# grid.arrange(merc, merc_map, nrow = 2)
merc_map
```

```{r, echo = FALSE, results = "hide"}
mean(nla_nomiss_both$total_hg)
# mean(nla_nomiss_both$RDis_IX)
```

Figure \ref{fig:figdata} shows that mercury concentration is right-skewed, with most lakes having a low value of mercury concentration but a few having a much higher concentration. Mercury concentration exhibits some spatial correlation, with high mercury concentrations in lakes in the northeast and north central United States. Because we are considering these lakes to be our entire population, we know that the realized mean mercury concentration is 103.03 ng / g.

```{r, results = "hide", fig.keep = "none"}
## IRS Sample, Model Analysis

set.seed(080421)
n <- 100
lakesobs <- nla_nomiss_both %>% sample_n(n) 
lakesunobs <- anti_join(nla_nomiss_both, lakesobs)
lakesunobs$total_hg <- NA

lakes_test <- bind_rows(lakesobs, lakesunobs) 
lakes_test$wts <- 1 / nrow(lakes_test) ## predicting for mean

slmfitout_exp_lakes <- slmfit(formula = total_hg ~ 1,
                              data = lakes_test, 
                              xcoordcol = 'xcoords',
                              ycoordcol = 'ycoords',
                              CorModel = "Exponential")
summary(slmfitout_exp_lakes)
plot(slmfitout_exp_lakes)

pred_exp_lakes <- predict(slmfitout_exp_lakes, wtscol = "wts",
                          conf_level = 0.95)
print(pred_exp_lakes)
mean_irs_mod <- pred_exp_lakes$FPBK_Prediction
se_irs_mod <- sqrt(pred_exp_lakes$PredVar)
lb_irs_mod <- pred_exp_lakes$conf_bounds[1]
ub_irs_mod <- pred_exp_lakes$conf_bounds[2]

realized_mean <- mean(nla_nomiss_both$total_hg) ## compute realized mean
```

```{r, results = "hide"}
## IRS Sample, Design Analysis
## 
irs_info <- lakesobs %>% summarise(meandoc = mean(total_hg),
                                   vardoc = var(total_hg))
N <- nrow(nla_nomiss_both); n <- nrow(lakesobs)
irs_se_irs_samp <- sqrt((irs_info$vardoc / n) * (N - n) / N)
irs_lb_irs_samp <- irs_info$meandoc - 1.96 * irs_se_irs_samp
irs_ub_irs_samp <- irs_info$meandoc + 1.96 * irs_se_irs_samp
irs_info$meandoc; sqrt(irs_info$vardoc)
irs_lb_irs_samp; irs_ub_irs_samp
```

```{r, results = "hide"}
## GRTS Sample, Design Analysis
data_sf <- sf::st_as_sf(nla_nomiss_both, coords = c("xcoords", "ycoords"),
                        crs = 5070)
library(spsurvey)
grts_samp <- grts(data_sf, n_base = n)
grts_bind <- sprbind(grts_samp)

## get coordinates
grts_coords <- sf::st_coordinates(grts_bind)
## make data frame
grts_df <- data.frame(
  response = grts_bind$total_hg,
  x = grts_coords[, "X"],
  y = grts_coords[, "Y"],
  siteID = grts_bind$siteID,
  wgt = grts_bind$wgt
)
## head(grts_df)
summary(grts_df$response)

design_analysis <- cont_analysis(
  grts_df,
  siteID = "siteID",
  vars = "response",
  weight = "wgt",
  xcoord = "x",
  ycoord = "y"
)
## just return the mean info
## design_mean <- subset(design_analysis$Pct, Statistic == "Mean")
design_mean_grts <- design_analysis$Mean
design_mean_grts$Estimate
design_mean_grts$StdError
design_mean_grts$LCB95Pct
design_mean_grts$UCB95Pct
```

```{r, results = "hide"}
## GRTS Sample, Model Analysis

grts_coords_resp <- grts_df %>% select(-siteID, -wgt) %>%
  rename(total_hg = "response", xcoords = "x", ycoords = "y")
grts_unsamp <- anti_join(nla_nomiss_both, grts_coords_resp)
grts_unsamp$total_hg <- NA
grts_unsamp <- grts_unsamp %>% select(total_hg, xcoords, ycoords)
grts_full <- dplyr::bind_rows(grts_coords_resp, grts_unsamp)

grts_full$wts <- 1 / nrow(grts_full)
mod_grts <- slmfit(total_hg ~ 1, data = grts_full,
                   xcoordcol = "xcoords",
                   ycoordcol = "ycoords")
pred_mod_grts <- predict(mod_grts, wtscol = "wts")
model_mean_grts <- pred_mod_grts$FPBK_Prediction
model_se_grts <- sqrt(pred_mod_grts$PredVar)
model_lb_grts <- model_mean_grts + -1 * 1.96 * model_se_grts
model_ub_grts <- model_mean_grts + 1 * 1.96 * model_se_grts
model_mean_grts
model_se_grts
model_lb_grts
model_ub_grts
```

```{r, results = "hide"}
## combine results
res_df <- tibble(approach = c("Design IRS", "Design GRTS", "Model IRS", "Model GRTS"),
       realized_mean = c(realized_mean, realized_mean, realized_mean,
                     realized_mean),
       estimate = c(irs_info$meandoc,
                    design_mean_grts$Estimate, as.vector(mean_irs_mod),
                    as.vector(model_mean_grts)),
       se = c(irs_se_irs_samp, design_mean_grts$StdError, as.vector(se_irs_mod),
              as.vector(model_se_grts)),
       lb = c(irs_lb_irs_samp, design_mean_grts$LCB95Pct, as.vector(lb_irs_mod),
              as.vector(model_lb_grts)),
       ub = c(irs_ub_irs_samp, design_mean_grts$UCB95Pct, as.vector(ub_irs_mod),
              as.vector(model_ub_grts)))
res_df <- res_df %>% slice(1, 3, 2, 4)
```

```{r apptab}
library(knitr)
names(res_df) <- c("Approach", "Realized Mean", "Estimate", "SE", "95% LB", "95% UB")
kable(res_df, digits = 1, caption = "\\label{tab:appliedtab} Table XXX. Application of design-based and model-based approaches to the NLA data set on mercury concentration.")
```

Table \ref{tab:appliedtab} shows the application of a design-based analysis on an IRS, a model-based analysis on an IRS, a design-based analysis on a GRTS sample, and a model-based analysis on a GRTS sample. We see that, for all four analyses, the true realized mean mercury concentration is within the bounds of the 95% intervals. However, we should not generalize the results of this particular realization to any other data set or even to other potential samples of this data set. 

But, we do note a couple of patterns. The design-based IRS analysis shows the largest standard error: a likely reason is that this is the only approach that does not use the spatial correlation in mercury concentration across the contiguous United States. We also see that, for the samples drawn, the both analyses with the GRTS sampling design have a lower standard error than the analyses with the IRS sampling design. We would expect this to be the case for most samples because mercury concentration exhibits spatial correlation so a spatially balanced sample should usually yield a lower standard error. If it is acceptable to have an interval for mean mercury concentration of about 25 ng / g and if we ignore the other variables that the EPA collects information on in these NLA surveys, then the EPA could consider sampling just 50 lakes to save time and money. 

# Discussion {#sec:discussion}

# References {#references .unnumbered}

