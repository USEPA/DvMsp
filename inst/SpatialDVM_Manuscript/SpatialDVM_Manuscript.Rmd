---
title: A comparison of design-based and model-based approaches for spatial data.
author:
  - name: Michael Dumelle
    email: Dumelle.Michael@epa.gov
    affiliation: USEPA
    footnote: 1
  - name: Matthew Higham
    email: mhigham@stlaw.edu
    affiliation: STLAW
    footnote: 1
  - name: Lisa Madsen
    affiliation: OSU
  - name: Anthony R. Olsen
    affiliation: USEPA
  - name: Jay M. Ver Hoef
    affiliation: NOAA
address:
  - code: USEPA
    address: United States Environmental Protection Agency, 200 SW 35th St, Corvallis, Oregon, 97333
  - code: STLAW
    address: Saint Lawrence University Department of Math, Computer Science, and Statistics, 23 Romoda Drive, Canton, New York, 13617
  - code: OSU
    address: Oregon State University Department of Statistics, 239 Weniger Hall, Corvallis, Oregon, 97331
  - code: NOAA
    address: Marine Mammal Laboratory, Alaska Fisheries Science Center, National Oceanic and Atmospheric Administration, Seattle, Washington, 98115
footnote:
  - code: 1
    text: "Corresponding Author"
abstract: |
  This is the abstract.

journal: "An awesome journal"
date: "`r Sys.Date()`"
bibliography: mybibfile.bib
linenumbers: true
numbersections: true
csl: elsevier-harvard.csl
preamble: >
  \usepackage{bm}
  \usepackage{bbm}
  \usepackage{color}
  \DeclareMathOperator{\var}{{var}}
  \DeclareMathOperator{\cov}{{cov}}
output: rticles::elsevier_article
---

_Text based on elsarticle sample manuscript, see [http://www.elsevier.com/author-schemas/latex-instructions#elsarticle](http://www.elsevier.com/author-schemas/latex-instructions#elsarticle)_

Potential Journals:

* Ecological Applications
* Methods in Ecology and Evolution
* Journal of Applied Ecology
* Environmetrics
* Environmental and Ecological Statistics

# Introduction {#sec:introduction}

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>", 
  warning = FALSE, 
  message = FALSE,
  include = TRUE, 
  echo = FALSE
)
library(tidyverse)
library(here)
library(xtable) # latex tables
```

<!-- Brief introduction to model based and design based -->

There are two general approaches for using data to make statistical inferences about a population: design-based approaches and model-based approaches. When data cannot be obtained for all units in a population (known as population units), data on a subset of the population units is collected in a  sample. In the design-based approach, inferences about the underlying population are informed from a probabilistic process in which population units are selected to be in the sample. Alternatively, in the model-based approach, inferences are made from specific assumptions made about the underlying process that generated the data. Each paradigm has a deep historical context [@sterba2009alternative] and its own set of general advantages [@hansen1983evaluation].

<!-- Shift focus to spatial data -->

Though the design-based and model-based approaches apply to statistical inference in a broad sense, we focus on comparing these approaches for spatial data. We define spatial data as variables measured at specific geographic locations. @de1990model give an early comparison of design-based and model-based approaches for spatial data, quashing the belief that design-based approaches could not be used for spatially correlated data. Thereafter, several comparisons between design-based and model-based for spatial data have been considered, but they tend to compare design-based approaches that ignore spatial location in sampling to model-based approaches [@brus1997random; @verhoef2002sampling; @verhoef2008spatial]. @cooper2006sampling review the two approaches in an ecological context before introducing a "model-assisted" variance estimator that combines aspects from each approach. In addition to @cooper2006sampling, there has been substantial research and development into estimators that use both design and model-based principles (see e.g. @cicchitelli2012model, @chan2020bayesian for a Bayesian approach, and @sterba2009alternative). More recent overviews include @brus2020statistical and @wang2012review, but no numerical comparison has been made between design-based approaches that incorporate spatial sampling and model-based approaches.

<!-- Outline for the rest of the paper -->

The rest of this paper is organized as follows. In Section \ref{sec:background}, we compare sampling and estimation procedures between the design-based approach and the model-based approach. In Section \ref{sec:numstudy}, we use simulated and real data to study the properties of parameter estimates from both approaches. And in Section \ref{sec:discussion}, we end with a discussion and provide directions for future research. 

# Background {#sec:background}

The design-based and model-based approaches incorporate randomness in fundamentally different ways. In this section, we describe the role of randomness and its effects on subsequent inferences. We then discuss specific inference methods for the design-based and model-based approaches for spatial data. 

## Comparing Design-Based vs. Model-Based

The design-based approach assumes the data are fixed. Randomness is incorporated in the selection of population units according to a sampling design. A sampling design assigns a positive probability of inclusion in the sample (inclusion probability) to each population unit. Some examples of commonly used sampling designs include independent random sampling (IRS), stratified random sampling, and cluster sampling. The goal is to use the sampling design and the sampled data to estimate population parameters like means and totals. These population parameters are typically assumed to be fixed but unknown. 

Treating the data as fixed and incorporating randomness through the sampling design yields estimators having very few other assumptions. Confidence intervals for these types of estimators are typically derived using limiting arguments. Means and totals, for example, are asymptotically normally distributed by the Central Limit Theorem. @sarndal2003model and @lohr2009sampling provide thorough reviews of the design-based approach.

The model-based approach assumes the data are a random realization of a process. Randomness is often incorporated through distributional assumptions on the data-generating process. Instead of estimating fixed but unknown parameters (as in the design-based approach), the goal of model-based inference in the spatial context is often \emph{prediction} of an unknown quantity. For example, suppose the realized mean of all population units is the quantity of interest. Instead of \emph{estimating} a fixed unknown mean, we are \emph{predicting} the value of the mean, a random variable. We know that if we sampled all population units, we would have an exact prediction for the mean of our one realized process, without any uncertainty. But the true mean of the spatial process that generated our realized data is still not known. When predicting the realized mean, we typically are not interested in the underlying process' true mean.

Assuming the data is a realization of a specific data-generating process yields predictors that are linked to distributional assumptions. These distributional assumptions are used to derive prediction intervals. The distributional assumptions allow the prediction intervals can be more precise. @cressie1993statistics and @schabenberger2017statistical provide reviews of model-based approaches for spatial data.

<!-- Figure 1a. Brus (2020): Data is fixed. In a finite population example, show a 3d surface that can be generated by anything. If we repeatedly sample the surface, then 95% of all 95% CIs will contain the true mean, which never changes. -->

```{r, out.width = "100%", fig.cap = "A comparison of sampling under the design-based and model-based frameworks. In the top row, we have one fixed population, and two random samples. In the bottom row, we have two realizations of the same spatial process sampled at the same locations."}
set.seed(06092021)

source(here("R", "sim_pop.R"))
N <- 25
pop_df <- sim_pop(N = N, gridded = TRUE, cortype = "Exponential",
                  psill = 0.9, erange = 1, nugget = 0.1)

n <- 5
samp_df <- pop_df %>% sample_n(n) 
samp_df2 <- pop_df %>% sample_n(n) 

library(cowplot)
d1 <- ggplot(data = pop_df, aes(x = x, y = y)) +
  geom_point(aes(colour = response), size = 4) +
  scale_colour_viridis_c() +
  geom_point(data = samp_df, size = 5.5, shape = 1, 
             show.legend = FALSE, stroke = 1.5) +
  theme_minimal_grid() +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
  ggtitle("Design 1")

d2 <- ggplot(data = pop_df, aes(x = x, y = y)) +
  geom_point(aes(colour = response), size = 4) +
  scale_colour_viridis_c() +
  geom_point(data = samp_df2, size = 5.5, shape = 1, show.legend = FALSE,
             stroke = 1.5) +
  theme_minimal_grid() +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
  ggtitle("Design 2")


pop_df_mod <- sim_pop(N = N, gridded = TRUE, cortype = "Exponential",
                      psill = 0.9, erange = 1, nugget = 0.1)
pop_df_mod2 <- sim_pop(N = N, gridded = TRUE, cortype = "Exponential",
                       psill = 0.9, erange = 1, nugget = 0.1)
indeces <- sample(1:N, size = n, replace = FALSE)
samp_df_mod <- pop_df_mod %>% slice(indeces)
samp_df_mod2 <- pop_df_mod2 %>% slice(indeces)

m1 <- ggplot(data = pop_df_mod, aes(x = x, y = y)) +
  geom_point(aes(colour = response), size = 4) +
  scale_colour_viridis_c() +
  geom_point(data = samp_df_mod, size = 5.5, shape = 1, show.legend = FALSE, stroke = 1.5) +
  theme_minimal_grid() +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
  ggtitle("Model 1")

m2 <- ggplot(data = pop_df_mod2, aes(x = x, y = y)) +
  geom_point(aes(colour = response), size = 4) +
  scale_colour_viridis_c() +
  geom_point(data = samp_df_mod2, size = 5.5, shape = 1, show.legend = FALSE, stroke = 1.5) +
  theme_minimal_grid() +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
  ggtitle("Model 2")

library(gridExtra)
grid.arrange(d1, d2, m1, m2)
```



<!-- Figure 1b. Spatial process is fixed. In a finite population example, show 10 3d surfaces that are generated from some model. If we repeatedly generate the surface and obtain a sample, then 95% of all 95% PIs will contain the realized means. The realized mean changes from surface to surface and it's not necessarily the case that 95% of all 95% PIs will contain the true, underlying mean. -->

## Spatially Balanced Design and Analysis

Sampling designs can incorporate spatial locations to obtain samples that are spatially balanced with respect to the population [@stevens2004spatially]. A sample is spatially with respect to the population if the sampled population units are a miniature of the population units. A sample is a miniature of the population if the distribution of the sampled population units mirrors the density of all population units. Spatial balance with respect to the population is is different than spatial balance with respect to geography. A sample that is spatially balanced with respect to geography is spread out in some type of equidistant manner over geographical space and are not meant to be miniatures of the population. When we refer to spatial balance henceforth, we mean spatial balance with respect to the population. 

Spatially balanced samples are useful because they tend to yield estimates that have lower variance than estimates constructed from sampling designs lacking spatial balance [@stevens2004spatially; @barabesi2011sampling; @grafstrom2013well; @robertson2013bas; @benedetti2017spatiallyreview; @wang2013design]. To quantify spatial balance, @stevens2004spatially proposed loss functions based on Voroni polygons. The first spatiallly balanced sampling algorithm that saw widespread use was the Generalized Random Tessellation Stratified [@stevens2004spatially]. Since GRTS was developed, several other spatially balanced sampling algorithms have emerged, including the Local Pivotal Method [@grafstrom2012spatially; @grafstrom2018spatially], Spatially Correlated Poisson Sampling [@grafstrom2012spatiallypoisson], Balanced Acceptance Sampling [@robertson2013bas], Within-Sample-Distance [@benedetti2017spatially], and Halton Iterative Partitioning [@robertson2018halton] algorithms. We focus on the Generalized Random Tessellation Stratified (GRTS) algorithm to select spatially balanced sampling because the algorithm has several attractive properties detailed by @stevens2004spatially and @dumelle2021spsurvey.

The GRTS algorithm is used to sample from finite and infinite populations and works by utilizing a mapping between two-dimensional and one-dimensional space. The population units in two-dimensional space are divided into cells using a hierarchical index. Population units are then mapped to a one-dimensional line via the hierarchical indexing. The line length of each population unit equals its inclusion probability. A systematic sample is conducted on the line and these samples are linked to a population unit in two-dimensional space, which results in the desired sample size. @stevens2004spatially provide and @dumelle2021spsurvey provide further details. The GRTS algorithm is available in the \textbf{\textsf{R}} package \texttt{spsurvey} [@dumelle2021spsurvey]. 

After collecting a sample, the data are used to estimate population parameters. The Horvitz-Thompson estimator [@horvitz1952generalization] and its continuous analog [@cordy1993extension] yield unbiased estimates of population means and totals. If $\tau$ is a population total, then the Horvitz-Thompson estimator of $\tau$ (denoted by $\hat{\tau}_{ht}$), is given by
\begin{align}\label{eq:ht}
  \hat{\tau}_{ht} = \sum_{i = 1}^n Z_i \pi_i^{-1},
\end{align}
where $Z_i$ and $pi_i$ are the observed value and inclusion probability of the $i$th population unit selected in the sample. @horvitz1952generalization and @sen1953estimate provide variance estimators for $\hat{\tau}_{ht}$, but they have two drawbacks. First, they rely on calculating $\pi_{ij}$, the probability that population unit $i$ and population unit $j$ are included in the sample, which can be very difficult to calculate. Second, they ignore the spatial locations of the population units. To address these drawbacks, @stevens2003variance proposed a local neighborhood variance estimator. The local neighborhood variance estimator does not rely on $\pi_{ij}$, and it incorporates spatial locations by assigning higher weights to nearby observations. @stevens2003variance show this variance estimators tends to reduce the variability associated with estimating $\tau$. This yields confidence intervals for $\tau$ that are narrower than confidence intervals constructed from variance estimators ignoring spatial locations.

## Finite Population Block Kriging

We only use FPBK in this paper in order to focus more on comparing the design-based and model-based approaches. However, k-nearest-neighbors [@fix1951discriminatory; @ver2013comparison], random forest [@breiman2001random], Bayesian models [@chan2020bayesian], among others, can also be used to obtain predictions for a mean or total from spatially correlated responses in a finite population setting.

<!-- \citep{VerHoef2002sampling, VerHoef2008spatial, Higham2020adjusting} -->

Finite Population Block Kriging (FPBK) is an alternative to samipling-based methods [@verhoef2008spatial]. FPBK expands the geostatistical kriging framework to the finite population setting. Instead of basing inference off of a specific sampling design, we assume the data were generated by a spatial process with parameters that can be estimated using the framework of a model. 

@verhoef2008spatial gives details on the theory of FPBK, but some of the basic principles are summarized below. For a response variable $\mathbf{z}$ that can be measured on a finite number of $N$ sites, we want to predict some linear function of the response variable, $\tau(\mathbf{z}) = \mathbf{b}^\prime \mathbf{z}$, where $\mathbf{b}$ is a vector of weights. For example, if we want to predict the total abundance across all sites, then we would use a vector of 1's for the weights. 

Typically, however, we only have a sample of the $N$ sites. Denoting quantities that are part of the sampled sites with a subscript \emph{s} and quantities that are part of the unsampled sites with a subscript \emph{u}, 

\begin{equation}
\begin{pmatrix} \label{equation:Zmarginal}
    \mathbf{z}_s      \\
    \mathbf{z}_u
\end{pmatrix}
=
\begin{pmatrix}
  \mathbf{X}_s    \\
  \mathbf{X}_u
\end{pmatrix}
\bm{\beta} +
\begin{pmatrix}
\bm{\delta}_s    \\
\bm{\delta}_u
\end{pmatrix},
\end{equation}
where $\mathbf{X}_s$ and $\mathbf{X}_u$ are the design matrices for the sampled and unsampled sites, respectively, and $\bm{\delta}_s$ and $\bm{\delta}_u$ are random errors for the sampled and unsampled sites. Denoting $\bm{\delta} \equiv [\bm{\delta}_s \,\, \bm{\delta}_u]'$, we assume that $E(\bm{\delta})$ = $\mathbf{0}$.

We also typically assume that there is spatial correlation in $\bm{\delta}$, which can be modeled using a covariance function. Many common choices for this function assume that spatial covariance decreases with increasing Euclidean distance between sites. The primary function used throughout the simulations and applications of this manuscript is the Exponential covariance function: the $i,j^{th}$ entry for $\var(\bm{\delta})$ is
\mbox{}
\begin{align}\label{equation:expcov}
\cov(\delta_i, \delta_j) = \theta_1\exp(-3h_{i,j}/\theta_2) + \theta_3\mathbbm{1}\{\mathbf{h}_{i,j} = 0\}, 
\end{align}
where $h_{i,j}$ is the distance between sites $i$ and $j$, and $\bm{\theta}$ is a vector of spatial covariance parameters of the partial sill $\theta_1$, the range $\theta_2$, and the nugget $\theta_3$. However, any spatial covariance function could be used in the place of the Exponential, including functions that allow for anisotropy [pg. 80 - 93][@chiles1999geostatistics].

With the above model formulation, the Best Linear Unbiased Predictor (BLUP) for $\tau(\mathbf{b}'\mathbf{z})$ $\tau(\mathbf{b}'\mathbf{z})$ is vague} and its prediction variance can be computed. While details of the derivation are in [@verhoef2008spatial], we note here that the predictor and its variance are both moment-based. Neither require a particular distribution for $\mathbf{z}$.

# Numerical Study {#sec:numstudy}

For the following simulation results, we simulated 1040 different gridded populations, each of size 900 with sample size 150. For the model-based approach (FPBK), sites were selected via Independent Random Sample. For GRTS, the local mean variance was used. 

The response was normally distributed with an exponential covariance function with partial sill of 0.9, effective range of $\sqrt{2}$, and a nugget of 0.1. For model-based, we assumed the correct form of the covariance function (Exponential), but estimated the spatial parameters with REML.

```{r, results = "asis"}
sim_one <- read_csv(here("inst", "output", "sim_one", "summ_output.csv"))
colnames(sim_one) <- c("Approach", "Bias", "RMSE", "MedAE", "Coverage", "PClose", "MedIL")
sim_one_table <- xtable(sim_one, digits = 4, caption = "Approach, mean bias (Bias), root-mean-squared error (RMSE), median absolute error (MedAE), 95 percent interval coverage (Coverage), proportion of times the approach was closer to the true value (PClose), and median interval length (MedIL)", type = "latex", latex.environments = "center", label = "tab:sim_one")
print(sim_one_table, include.rownames = FALSE, comment = FALSE)
```

__Base Simulations__

* both good: correctly specified model with high correlation
* break model: non-gaussian errors
* break design: small area estimation

* both good?: misspecified covariance model with high correlation
* break both? non-gaussian areas with smaller sample size

* change n or sampling fraction

* model-based: how should sample be drawn? should locations be fixed?

## Software

FPBK can be readily performed in `R` with the `sptotal` package [@higham2020sptotal]. We use `sptotal` for both the simulation analysis and the application, estimating parameters with Restricted Maximum Likelihood (REML).

## Applied Example

Potential Data Sets:

* National Lakes Assessment 
* Moose in Alaska
* Temperature Data from NOAA

# Discussion {#sec:discussion}

<!-- References from Brus2020statistical, de1990model, Brus1997random -->

# References {#references .unnumbered}

