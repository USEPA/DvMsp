---
title: "Application with Different Lakes Data"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(here)
library(sptotal)
library(skimr)
nla_df <- read_csv(here("data", "nla12_keyvariables_data.csv"))
skim(nla_df)
```

```{r}
View(nla_df)
factor(nla_df$TOTALHG_UNITS) ## all same units
```

```{r}
nla_df %>% group_by(SITE_ID) %>%
  summarise(nonmiss = sum(!is.na(TOTALHG_RESULT))) %>%
  ungroup() %>% summarise(maxnonmiss = max(nonmiss))

nla_df %>% group_by(SITE_ID) %>%
  summarise(nonmiss = sum(!is.na(TOTALHG_RESULT))) %>%
  summarise(sumnomiss = sum(nonmiss == 0))
```

HG never measured twice at the same site. 35 sites never have HG measured. These will be dropped as well as the duplicate sites.

```{r}
nla_nomiss <- nla_df %>% filter(!is.na(TOTALHG_RESULT)) %>%
  rename(total_hg = "TOTALHG_RESULT")

## check
nla_nomiss %>% group_by(SITE_ID) %>% count() %>%
  ungroup() %>%
  summarise(not1 = sum(n != 1))
```

Exploratory graph

```{r}
ggplot(data = nla_nomiss, aes(x = total_hg)) +
  geom_histogram(bins = 20)
```

Exploratory summary stats

```{r}
summary(nla_nomiss$total_hg)
```

```{r}
ggplot(data = nla_nomiss, aes(x = INDEX_LON_DD, y = INDEX_LAT_DD)) +
  geom_point(aes(colour = total_hg)) +
  scale_colour_viridis_c()
```

We see some spatial correlation (higher values in northeast, north, south, and somewhat high on the western coast. lower values in middle america, where it tends to be more rural).

transform coordinates

```{r}
coord_list <- LLtoTM(cm = mean(nla_nomiss$INDEX_LON_DD), lat = nla_nomiss$INDEX_LAT_DD,
       lon = nla_nomiss$INDEX_LON_DD)
nla_nomiss$xcoords <- coord_list$xy[ ,1]
nla_nomiss$ycoords <- coord_list$xy[ ,2]
ggplot(data = nla_nomiss, aes(x = xcoords, y = ycoords)) +
  geom_point(aes(colour = total_hg)) +
  scale_colour_viridis_c()
```

## IRS Sample, Model Analysis

```{r}
set.seed(07262021)
lakesobs <- nla_nomiss %>% sample_n(200) ## sample 200 lakes
lakesunobs <- anti_join(nla_nomiss, lakesobs)
lakesunobs$total_hg <- NA

lakes_test <- bind_rows(lakesobs, lakesunobs) 
lakes_test$wts <- 1 / nrow(lakes_test) ## predicting for mean
```

```{r}
slmfitout_exp_lakes <- slmfit(formula = total_hg ~ 1,
                              data = lakes_test, 
                              xcoordcol = 'xcoords',
                              ycoordcol = 'ycoords',
                              CorModel = "Exponential")
summary(slmfitout_exp_lakes)
plot(slmfitout_exp_lakes)
```

```{r}
pred_exp_lakes <- predict(slmfitout_exp_lakes, wtscol = "wts",
                          conf_level = 0.95)
print(pred_exp_lakes)
mean_irs_mod <- pred_exp_lakes$FPBK_Prediction
se_irs_mod <- sqrt(pred_exp_lakes$PredVar)
lb_irs_mod <- pred_exp_lakes$conf_bounds[1]
ub_irs_mod <- pred_exp_lakes$conf_bounds[2]


realized_mean <- mean(nla_nomiss$total_hg) ## compute realized mean
```

## IRS Sample, Design Analysis

```{r}
irs_info <- lakesobs %>% summarise(meandoc = mean(total_hg),
                                   vardoc = var(total_hg))
N <- nrow(nla_nomiss); n <- nrow(lakesobs)
irs_se_irs_samp <- sqrt((irs_info$vardoc / n) * (N - n) / N)
irs_lb_irs_samp <- irs_info$meandoc - 1.96 * irs_se_irs_samp
irs_ub_irs_samp <- irs_info$meandoc + 1.96 * irs_se_irs_samp
irs_info$meandoc; sqrt(irs_info$vardoc)
irs_lb_irs_samp; irs_ub_irs_samp
```

## GRTS Sample, Design Analysis

Next, we perform design-based and model-based analyses using a GRTS sample of 200 sites. The following code obtains the GRTS sample.

```{r, results = "hide"}
set.seed(07262021)
data_sf <- sf::st_as_sf(nla_nomiss, coords = c("xcoords", "ycoords"),
                        crs = 5070)
library(spsurvey)
grts_samp <- grts(data_sf, n_base = n)
grts_bind <- sprbind(grts_samp)

## get coordinates
grts_coords <- sf::st_coordinates(grts_bind)
## make data frame
grts_df <- data.frame(
  response = grts_bind$total_hg,
  x = grts_coords[, "X"],
  y = grts_coords[, "Y"],
  siteID = grts_bind$siteID,
  wgt = grts_bind$wgt
)
## head(grts_df)
summary(grts_df$response)
```

We then perform a design-based analysis on the GRTS sample.

```{r}
design_analysis <- cont_analysis(
  grts_df,
  siteID = "siteID",
  vars = "response",
  weight = "wgt",
  xcoord = "x",
  ycoord = "y"
)
## just return the mean info
## design_mean <- subset(design_analysis$Pct, Statistic == "Mean")
design_mean_grts <- design_analysis$Mean
design_mean_grts$Estimate
design_mean_grts$StdError
design_mean_grts$LCB95Pct
design_mean_grts$UCB95Pct
```

## GRTS Sample, Model Analysis

Finally, we use a model-based analysis on the GRTS sample. 

```{r}
grts_coords_resp <- grts_df %>% select(-siteID, -wgt) %>%
  rename(total_hg = "response", xcoords = "x", ycoords = "y")
grts_unsamp <- anti_join(nla_nomiss, grts_coords_resp)
grts_unsamp$total_hg <- NA
grts_unsamp <- grts_unsamp %>% select(total_hg, xcoords, ycoords)
grts_full <- dplyr::bind_rows(grts_coords_resp, grts_unsamp)
```

```{r}
grts_full$wts <- 1 / nrow(grts_full)
mod_grts <- slmfit(total_hg ~ 1, data = grts_full,
                   xcoordcol = "xcoords",
                   ycoordcol = "ycoords")
pred_mod_grts <- predict(mod_grts, wtscol = "wts")
model_mean_grts <- pred_mod_grts$FPBK_Prediction
model_se_grts <- sqrt(pred_mod_grts$PredVar)
model_lb_grts <- model_mean_grts + -1 * 1.96 * model_se_grts
model_ub_grts <- model_mean_grts + 1 * 1.96 * model_se_grts
model_mean_grts
model_se_grts
model_lb_grts
model_ub_grts
```

Combine all results:

```{r}
res_df <- tibble(approach = c("Design IRS", "Design GRTS", "Model IRS", "Model GRTS"),
       realized_mean = c(realized_mean, realized_mean, realized_mean,
                     realized_mean),
       estimate = c(irs_info$meandoc,
                    design_mean_grts$Estimate, as.vector(mean_irs_mod),
                    as.vector(model_mean_grts)),
       se = c(irs_se_irs_samp, design_mean_grts$StdError, as.vector(se_irs_mod),
              as.vector(model_se_grts)),
       lb = c(irs_lb_irs_samp, design_mean_grts$LCB95Pct, as.vector(lb_irs_mod),
              as.vector(model_lb_grts)),
       ub = c(irs_ub_irs_samp, design_mean_grts$UCB95Pct, as.vector(ub_irs_mod),
              as.vector(model_ub_grts)))
res_df$estimate
```

All 95% CI's contain the true mean for this one simulation. 

For this simulation, in terms of standard error:

Model GRTS < Design GRTS < Model IRS < Design IRS

In terms of how close the prediction is to the true mean (prediction - true_mean), from closest to furthest:

Model IRS < Design IRS < Model GRTS < Design GRTS

```{r}
## irs sample
ggplot(lakesobs, aes(x = total_hg)) +
         geom_histogram(colour  = "black", fill = "white", bins = 20)

## grts sample
ggplot(grts_df, aes(x = response)) +
         geom_histogram(colour  = "black", fill = "white", bins = 20)
```

Candidates for symmetric variable.

```{r}
## disturbance for benthic microinvertebrates
ggplot(data = nla_df, aes(x = MMI_BENT_NLA12)) +
  geom_histogram(colour = "black", fill = "white", bins = 20)

## disturbance for zooplankton
ggplot(data = nla_df, aes(x = MMI_ZOOP_NLA6)) +
  geom_histogram(colour = "black", fill = "white", bins = 20)

## lakeshore disturbance indicator
ggplot(data = nla_df, aes(x = RDis_IX)) +
  geom_histogram(colour = "black", fill = "white", bins = 20)
```

But, does it make sense to want to find an average for any of these?





